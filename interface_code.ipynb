{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import warnings \n",
    "import streamlit as st\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the data\n",
    "file = pd.read_csv('heloc_dataset_v1.csv')\n",
    "file = file.replace([-9], np.nan)\n",
    "file = file.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = file.copy().drop('RiskPerformance',axis = 1)\n",
    "y = file['RiskPerformance']\n",
    "# categorical_MaxDelq2PublicRecLast12M = pd.get_dummies(X['MaxDelq2PublicRecLast12M'])\n",
    "# categorical_MaxDelqEver = pd.get_dummies(X['MaxDelqEver'])\n",
    "# file_categorical = pd.concat((categorical_MaxDelq2PublicRecLast12M,categorical_MaxDelqEver),axis=1,ignore_index = True)\n",
    "# file_numeric = X.copy().drop(['MaxDelq2PublicRecLast12M','MaxDelqEver'],axis = 1)\n",
    "# X = pd.concat((file_numeric,file_categorical),axis = 1)\n",
    "y = pd.factorize(y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_train, X_test = train_test_split(X, test_size=0.25, random_state=1)\n",
    "y_train, y_test = train_test_split(y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to fit the data \n",
    "pipe_model = Pipeline([('standardscaler', StandardScaler()), ('model',\n",
    "            AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
    "                   n_estimators=50, random_state=1) )])\n",
    "pipe_model.fit(X_train,y_train)\n",
    "# Save the data and pipeline\n",
    "pickle.dump(X_train, open('X_train.sav', 'wb'))\n",
    "pickle.dump(pipe_model, open('pipe_model.sav', 'wb'))\n",
    "pickle.dump(X_test, open('X_test.sav', 'wb'))\n",
    "pickle.dump(y_test, open('y_test.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING USING PIPELINE UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline and data\n",
    "pipe = pickle.load(open('pipe_model.sav', 'rb'))\n",
    "X_test = pickle.load(open('X_test.sav', 'rb'))\n",
    "y_test = pickle.load(open('y_test.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {0: 'Bad', 1: 'Good'}\n",
    "\n",
    "\n",
    "#Function to test certain index of dataset\n",
    "def test_demo(index):\n",
    "    values = X_test.iloc[index]  # Input the value from dataset\n",
    "\n",
    "    # Create sliders in the sidebar\n",
    "    \n",
    "    sidebars = [\n",
    "        st.sidebar.slider('ExternalRiskEstimate', 0.0, 100.0, values['ExternalRiskEstimate'], 1.0),\n",
    "        st.sidebar.slider('MSinceOldestTradeOpen', 0.0, 1000.0, values['MSinceOldestTradeOpen'], 1.0),\n",
    "        st.sidebar.slider('MSinceMostRecentTradeOpen', 0.0, 500.0, values['MSinceMostRecentTradeOpen'], 1.0),\n",
    "        st.sidebar.slider('AverageMInFile', 0.0, 500.0, values['AverageMInFile'], 1.0),\n",
    "        st.sidebar.slider('NumSatisfactoryTrades', 0.0, 100.0, values['NumSatisfactoryTrades'], 1.0),\n",
    "        st.sidebar.slider('NumTrades60Ever2DerogPubRec', 0.0, 100.0, values['NumTrades60Ever2DerogPubRec'], 1.0),\n",
    "        st.sidebar.slider('NumTrades90Ever2DerogPubRec', 0.0, 100.0, values['NumTrades90Ever2DerogPubRec'], 1.0),\n",
    "        st.sidebar.slider('PercentTradesNeverDelq', 0.0, 100.0, values['PercentTradesNeverDelq'], 1.0),\n",
    "        st.sidebar.slider('MSinceMostRecentDelq', -8.0, 100.0, values['MSinceMostRecentDelq'], 1.0),\n",
    "        st.sidebar.slider('NumTotalTrades', 0.0, 200.0, values['NumTotalTrades'], 1.0),\n",
    "        st.sidebar.slider('NumTradesOpeninLast12M', 0.0, 100.0, values['NumTradesOpeninLast12M'], 1.0),\n",
    "        st.sidebar.slider('PercentInstallTrades', 0.0, 200.0, values['PercentInstallTrades'], 1.0),\n",
    "        st.sidebar.slider('MSinceMostRecentInqexcl7days', -8.0, 50.0, values['MSinceMostRecentInqexcl7days'], 1.0),\n",
    "        st.sidebar.slider('NumInqLast6M', 0.0, 100.0, values['NumInqLast6M'], 1.0),\n",
    "        st.sidebar.slider('NumInqLast6Mexcl7days', 0.0, 100.0, values['NumInqLast6Mexcl7days'], 1.0),\n",
    "        st.sidebar.slider('NetFractionRevolvingBurden', -8.0, 500.0, values['NetFractionRevolvingBurden'], 1.0),\n",
    "        st.sidebar.slider('NetFractionInstallBurden', -8.0, 500.0, values['NetFractionInstallBurden'], 1.0),\n",
    "        st.sidebar.slider('NumRevolvingTradesWBalance', -8.0, 100.0, values['NumRevolvingTradesWBalance'], 1.0),\n",
    "        st.sidebar.slider('NumInstallTradesWBalance', -8.0, 100.0, values['NumInstallTradesWBalance'], 1.0),\n",
    "        st.sidebar.slider('NumBank2NatlTradesWHighUtilization', -8.0, 100.0, values['NumBank2NatlTradesWHighUtilization'], 1.0),\n",
    "        st.sidebar.slider('PercentTradesWBalance', -8.0, 100.0, values['PercentTradesWBalance'], 1.0),                   \n",
    "        st.sidebar.slider('MaxDelq2PublicRecLast12M', -8.0, 100.0, values['MaxDelq2PublicRecLast12M'], 1.0),\n",
    "        st.sidebar.slider('MaxDelqEver', -8.0, 100.0, values['MaxDelqEver'], 1.0)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    #Print the prediction result\n",
    "    alg = ['Best model','Decision Tree', 'Random Forest','Support Vector Machine', 'KNN','Logistic Regression']\n",
    "    classifier = st.selectbox('Which algorithm?', alg)\n",
    "\n",
    "    if classifier == 'Decision Tree':\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_test_s= scaler.transform(X_test)\n",
    "        dtc = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
    "                       max_features=None, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=3,\n",
    "                       min_weight_fraction_leaf=0.0, presort=False,\n",
    "                       random_state=1, splitter='best')\n",
    "        dtc.fit(X_train_s, y_train)\n",
    "        acc = dtc.score(X_test_s, y_test)\n",
    "        res = dtc.predict(np.array(sidebars)).reshape(1, -1)[0]\n",
    "        st.write('Prediction:  ', dic[res])       \n",
    "        st.write('Accuracy: ', acc)\n",
    "        pred = dtc.predict(X_test_s)\n",
    "        cm = metrics.confusion_matrix(y_test, pred)\n",
    "        st.write('Confusion matrix: ', cm)\n",
    "        st.write('Precision: ',metrics.precision_score(y_test,pred, average='macro'))\n",
    "        st.write('Recall: ', metrics.recall_score(y_test,pred, average='macro'))\n",
    "    \n",
    "\n",
    "    elif classifier == 'Support Vector Machine':\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_test_s= scaler.transform(X_test)\n",
    "        svm = SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "                  kernel='linear', max_iter=-1, probability=False, random_state=1,\n",
    "                  shrinking=True, tol=0.001, verbose=False)\n",
    "        svm.fit(X_train_s, y_train)\n",
    "        acc = svm.score(X_test_s, y_test)\n",
    "        res = svm.predict(np.array(sidebars).reshape(1, -1))[0]\n",
    "        st.write('Prediction:  ', dic[res])       \n",
    "        st.write('Accuracy: ', acc)\n",
    "        pred = svm.predict(X_test_s)\n",
    "        cm = metrics.confusion_matrix(y_test, pred)\n",
    "        st.write('Confusion matrix: ', cm)\n",
    "        st.write('Precision: ',metrics.precision_score(y_test,pred, average='macro'))\n",
    "        st.write('Recall: ', metrics.recall_score(y_test,pred, average='macro'))\n",
    "    \n",
    "    elif classifier == 'Random Forest':\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_test_s= scaler.transform(X_test) \n",
    "        rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=0.2, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=30,\n",
    "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
    "                       warm_start=False)\n",
    "        rf.fit(X_train_s, y_train)\n",
    "        acc = rf.score(X_test_s, y_test)\n",
    "        res = rf.predict(np.array(sidebars).reshape(1, -1))[0]\n",
    "        st.write('Prediction:  ', dic[res])       \n",
    "        st.write('Accuracy: ', acc)\n",
    "        pred = rf.predict(X_test_s)\n",
    "        cm = metrics.confusion_matrix(y_test, pred)\n",
    "        st.write('Confusion matrix: ', cm)\n",
    "        st.write('Precision: ',metrics.precision_score(y_test,pred, average='macro'))\n",
    "        st.write('Recall: ', metrics.recall_score(y_test,pred, average='macro'))\n",
    "    \n",
    "    elif classifier == 'KNN':\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_test_s= scaler.transform(X_test)\n",
    "        knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                                   metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
    "                                   weights='uniform')\n",
    "        knn.fit(X_train_s, y_train)\n",
    "        acc = knn.score(X_test_s, y_test)\n",
    "        res = knn.predict(np.array(sidebars).reshape(1, -1))[0]\n",
    "        st.write('Prediction:  ', dic[res])       \n",
    "        st.write('Accuracy: ', acc)\n",
    "        pred = knn.predict(X_test_s)\n",
    "        cm = metrics.confusion_matrix(y_test, pred)\n",
    "        st.write('Confusion matrix: ', cm)\n",
    "        st.write('Precision: ',metrics.precision_score(y_test,pred, average='macro'))\n",
    "        st.write('Recall: ', metrics.recall_score(y_test,pred, average='macro'))  \n",
    "        \n",
    "    elif classifier == 'Logistic Regression':\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_test_s= scaler.transform(X_test)\n",
    "        lr = LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
    "                                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                                   multi_class='warn', n_jobs=None, penalty='l1',\n",
    "                                   random_state=1, solver='warn', tol=0.0001, verbose=0,\n",
    "                                   warm_start=False)\n",
    "        lr.fit(X_train_s, y_train)\n",
    "        acc = lr.score(X_test_s, y_test)\n",
    "        res = lr.predict(np.array(sidebars).reshape(1, -1))[0]\n",
    "        st.write('Prediction:  ', dic[res])       \n",
    "        st.write('Accuracy: ', acc)\n",
    "        pred = lr.predict(X_test_s)\n",
    "        cm = metrics.confusion_matrix(y_test, pred)\n",
    "        st.write('Confusion matrix: ', cm)\n",
    "        st.write('Precision: ',metrics.precision_score(y_test,pred, average='macro'))\n",
    "        st.write('Recall: ', metrics.recall_score(y_test,pred, average='macro'))\n",
    "\n",
    "    else:\n",
    "        pipe = pickle.load(open('pipe_model.sav', 'rb'))\n",
    "        res = pipe.predict(np.array(sidebars).reshape(1, -1))[0]\n",
    "        st.write('Prediction:  ', dic[res])\n",
    "        st.write('Best Classifier Name: ', 'AdaBoost')\n",
    "        best_model = '''AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
    "                   n_estimators=50, random_state=1)'''\n",
    "        st.write('Best Model: ',best_model)\n",
    "        pred = pipe.predict(X_test)\n",
    "        score = pipe.score(X_test, y_test)\n",
    "        cm = metrics.confusion_matrix(y_test, pred)\n",
    "        st.write('Accuracy: ', score)\n",
    "        st.write('Confusion Matrix: ', cm)\n",
    "        st.write('Precision: ',metrics.precision_score(y_test,pred, average='macro'))\n",
    "        st.write('Recall: ', metrics.recall_score(y_test,pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title\n",
    "st.title('The risk of HELOC')\n",
    "# show data\n",
    "if st.checkbox('Show dataframe'):\n",
    "    st.write(X_test)\n",
    "# st.write(X_train) # Show the dataset\n",
    "\n",
    "number = st.text_input('Choose a row of information in the dataset (0~2466):', 5)  # Input the index number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demo(int(number))  # Run the test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
